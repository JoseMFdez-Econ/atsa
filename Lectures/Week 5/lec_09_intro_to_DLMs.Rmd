---
title: "Dynamic Linear Models"
subtitle: "FISH 507 â€“ Applied Time Series Analysis"
author: "Mark Scheuerell"
date: "5 Feb 2019"
output:
  ioslides_presentation:
    css: lecture_slides.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(kableExtra)
set.seed(123)
```

## Topics for today




## Simple linear regression

Let's begin with a linear regression model

$$
y_i = \alpha + \beta x_i + e_i ~ \text{with} ~ e_i \sim \text{N}(0,\sigma^2)
$$

The index $i$ has no explicit meaning--shuffling ($y_i,x_i$) pairs has no effect on parameter estimation


## Simple linear regression

We can write the model in matrix form

$$
y_i = \alpha + \beta x_i + e_i \\
\Downarrow \\
y_i = 
\begin{bmatrix}
  1 & x_i
\end{bmatrix}
\begin{bmatrix}
  \alpha \\
  \beta
\end{bmatrix} +
e_i
$$


## Simple linear regression

We can write the model in matrix form

$$
y_i = \alpha + \beta x_i + e_i \\
\Downarrow \\
y_i = 
\begin{bmatrix}
  1 & x_i
\end{bmatrix}
\begin{bmatrix}
  \alpha \\
  \beta
\end{bmatrix} +
e_i \\
\Downarrow \\
y_i = \mathbf{X}^{\top}_i \mathbf{\Theta} + e_i
$$

with $\mathbf{X}^{\top}_i = \begin{bmatrix} 1 & x_i \end{bmatrix}$ and $\mathbf{\Theta} = \begin{bmatrix} \alpha & \beta \end{bmatrix}^{\top}$ 


## Dynamic linear model (DLM)

In a _dynamic_ linear model, the regression parameters change over time, so we write

$$
y_i = \mathbf{X}^{\top}_i \mathbf{\Theta} + e_i ~~~~~~~ \text{(static)}
$$

as

$$
y_t = \mathbf{X}^{\top}_t \mathbf{\Theta}_t + e_t ~~~~~~~ \text{(dynamic)}
$$


## Dynamic linear model (DLM)

There are 2 important points here:

$$
y_\boxed{t} = \mathbf{X}^{\top}_t \mathbf{\Theta}_t + e_t
$$

1. Subscript $t$ explicitly acknowledges implicit info in the time ordering of the data in $\mathbf{y}$  


## Dynamic linear model (DLM)

There are 2 important points here:

$$
y_t = \mathbf{X}^{\top}_t \mathbf{\Theta}_\boxed{t} + e_t
$$

1. Subscript $t$ explicitly acknowledges implicit info in the time ordering of the data in $\mathbf{y}$  

2. The relationship between $\mathbf{y}$ and $\mathbf{X}$ is unique for every $t$


## Constraining a DLM

Close examination of the DLM reveals an apparent problem for parameter estimation

$$
y_t = \mathbf{X}^{\top}_t \mathbf{\Theta}_\boxed{t} + e_t
$$


## Constraining a DLM

Close examination of the DLM reveals an apparent problem for parameter estimation

$$
y_t = \mathbf{X}^{\top}_t \mathbf{\Theta}_t + e_t
$$

We only have 1 data point per time step (ie, $y_t$ is a scalar)

__Thus, we can only estimate 1 parameter (with no uncertainty)!__


## Constraining a DLM

To address this issue, we'll constrain the regression parameters to be dependent from $t$ to $t+1$

$$
\mathbf{\Theta}_t = \mathbf{G}_t \mathbf{\Theta}_{t-1} + \mathbf{w}_t ~ \text{with} ~ \mathbf{w}_t \sim \text{MVN}(\mathbf{0}, \mathbf{Q})
$$


## Constraining a DLM

In practice, we often make $\mathbf{G}_t$ time invariant 

$$
\mathbf{\Theta}_t = \mathbf{G} \mathbf{\Theta}_{t-1} + \mathbf{w}_t
$$

or assume $\mathbf{G}_t$ is the identity matrix $\mathbf{I}$

$$
\begin{align}
  \mathbf{\Theta}_t &= \mathbf{I} \mathbf{\Theta}_{t-1} + \mathbf{w}_t \\
                    &= \mathbf{\Theta}_{t-1} + \mathbf{w}_t
\end{align}
$$

In the latter case, the parameters follow a random walk over time


## A very simple model

Consider this simple model, consisting of a mean $\mu$ plus error

$$
y_i = \mu + e_i ~ \text{with} ~ e_i \sim \text{N}(0,\sigma^2)
$$


## A very simple model

The right-hand side of the equation is composed of _deterministic_ and _stochastic_ pieces

$$
y_i = \underbrace{\mu}_{\text{deterministic}} + \underbrace{e_i}_{\text{stochastic}}
$$


## A very simple model

Sometime these pieces are referred to as _fixed_ and _random_

$$
y_i = \underbrace{\mu}_{\text{fixed}} + \underbrace{e_i}_{\text{random}}
$$


## A very simple model

This can also be seen by rewriting the model 

$$
y_i = \mu + e_i ~ \text{with} ~ e_i \sim \text{N}(0,\sigma^2)
$$

as

$$
y_i \sim \text{N}(\mu,\sigma^2)
$$


## Simple linear regression

We can expand the deterministic part of the model, as with linear regression

$$
y_i = \underbrace{\alpha + \beta x_i}_{\text{mean}} + e_i ~ \text{with} ~ e_i \sim \text{N}(0,\sigma^2)
$$

so

$$
y_i \sim \text{N}(\alpha + \beta x_i,\sigma^2)
$$


## A simple time series model

Consider a simple model with a mean $\mu$ plus white noise

$$
y_t = \mu + e_t ~ \text{with} ~ e_t \sim \text{N}(0,\sigma^2)
$$


## Time series model with covariates

We can expand the deterministic part of the model, as before with linear regression

$$
y_t = \underbrace{\alpha + \beta x_t}_{\text{mean}} + e_t ~ \text{with} ~ e_t \sim \text{N}(0,\sigma^2)
$$

so

$$
y_t \sim \text{N}(\alpha + \beta x_t,\sigma^2)
$$



## Model with autocorrelated errors

We can expand the stochastic part of the model to have autocorrelated errors

$$
y_t = \alpha + \beta x_t + e_t \\
e_t = \phi e_{t-1} + w_t
$$

with $w_t \sim \text{N}(0,\sigma^2)$


## Model with autocorrelated errors

We can expand the stochastic part of the model to have autocorrelated errors

$$
y_t = \alpha + \beta x_t + e_t \\
e_t = \phi e_{t-1} + w_t
$$

with $w_t \sim \text{N}(0,\sigma^2)$

We can write this model as our standard state-space model


## State-space model | Observation equation

$$
\begin{align}
  y_t &= \alpha + \beta x_t + e_t \\
      &= e_t + \alpha + \beta x_t\\
      &\Downarrow \\
  y_t &= x_t + a + D d_t + v_t\\
\end{align}
$$

with

$x_t = e_t$, $a = \alpha$, $D = \beta$, $d_t = x_t$, $v_t = 0$


## State-space model | State equation


$$
\begin{align}
  e_t &= \phi e_{t-1} + w_t \\
      &\Downarrow \\
  x_t &= B x_t + w_t\\
\end{align}
$$

with

$x_t = e_t$ and $B = \phi$


## State-space model | Full form

$$
y_t = \alpha + \beta x_t + e_t \\
e_t = \phi e_{t-1} + w_t \\
\Downarrow \\
y_t = a + D d_t + x_t\\
x_t = B x_t + w_t
$$



## {.flexbox .vcenter}

<font size="10">MORE RANDOM EFFECTS</font>




## Random walk observed with error

$$
y_t = \mu + f_t + e_t ~ \text{with} ~ e_t \sim \text{N}(0, \sigma) \\
f_t = f_{t-1} + w_t ~ \text{with} ~ w_t \sim \text{N}(0, \gamma) \\
\Downarrow \\
y_t = a + x_t + v_t ~ \text{with} ~ v_t \sim \text{N}(0, R) \\
x_t = x_{t-1} + w_t ~ \text{with} ~ w_t \sim \text{N}(0, Q)
$$


## {.flexbox .vcenter}

<font size="10">MULTIPLE TIME SERIES</font>



## Random walk observed with error | In matrix form

$$
\mathbf{y}_t = \mathbf{x}_t + \mathbf{a} + \mathbf{v}_t \\
\mathbf{x}_t = \mathbf{x}_{t-1} + \mathbf{w}_t
$$

with 

$\mathbf{v}_t \sim \text{MVN}(\mathbf{0}, \mathbf{R})$

$\mathbf{w}_t \sim \text{MVN}(\mathbf{0}, \mathbf{Q})$



