---
title: "Semi- and non-parametric time series models"
author: "Eric Ward"
date: "26 Feb 2019"
output:
  ioslides_presentation:
    css: lecture_slides.css
  beamer_presentation: default
subtitle: FISH 507 â€“ Applied Time Series Analysis
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, 
                      message=FALSE, results='hide',
                      fig.align = 'center')
library(ggplot2)
library(MARSS)
library(dplyr)
```

## Overview of today's material

* Gaussian processes for time series

* Neural networks

* EDM 

## Gaussian processes for time series

Last week, we discussed exponential smoothing and in lab touched on GAMs

* Both approaches are similar in that they borrow information from neighbors
* Exponential smoothing usually borrows information from past data for forecasting
* Generalized additive models (GAMs) usually borrow information from both future and past data

## Gaussian processes for time series

GAMs estimate the *trend* using a smooth function, 

$$E\left[ Y \right] ={ B }_{ 0 }+f(x)$$

where like regression, we assume $Y\sim Normal\left( E\left[ Y \right] ,\sigma  \right)$

* The smooth function approximates the trend at a smaller subset of locations (aka *knots*)
* The density and location of the knots can affect how 'wiggly' the function is

## Gaussian processes for time series

For data that are regularly spaced in time, this probably isn't a big deal

* For instance if we use a cubic spline (default) on the 'airmiles' dataset (n = 23), a function approximating the trend is estimated at 10 equally spaced locations (grey vertical lines).

```{r echo =FALSE, fig.height=3.5, fig.width=3.5}
data(airmiles)
df = data.frame("y"=airmiles, "t"=1937:1960)
gam_mod = mgcv::gam(y ~ s(t, bs='cr'), data=df)
df$pred = predict(gam_mod)
knots = data.frame("x"=gam_mod$smooth[[1]]$xp)
ggplot(df, aes(t,y)) + geom_point(col="red") +
  geom_line(aes(t,pred)) + 
  geom_vline(data=knots, aes(xintercept=x), col="grey70")
```

* This fit looks pretty reasonable

## Gaussian processes for time series

Let's try again, this time knocking a few holes in the data. Removing years 1950:1953 and 1955:1959, the knot locations are no longer equally spaced, and weighted more toward the locations of data points. 

* Greater spacing between knots = less flexibility, more uncertainty (you can look at the 'se.fit' part of predict output)

```{r echo =FALSE, fig.height=3.5, fig.width=3.5}
data(airmiles)
df = data.frame("y"=airmiles, "t"=1937:1960)
df = dplyr::filter(df,t %in% c(1950:1953,1955:1959)==FALSE)
gam_mod = mgcv::gam(y ~ s(t, bs='cr'), data=df)
df$pred = predict(gam_mod)
knots = data.frame("x"=gam_mod$smooth[[1]]$xp)
ggplot(df, aes(t,y)) + geom_point(col="red") +
  geom_line(aes(t,pred)) + geom_vline(data=knots, aes(xintercept=x), col="grey70")
```

## Gaussian processes for time series

Recapping, GAMs are estimating the underlying *trend* using a smooth function, 

$$E\left[ Y \right] ={ B }_{ 0 }+f(x)$$

* It's important to note that this underlying trend function $f(x)$ is modeling the **mean**
* Smooths are very flexible (with respect to # knots, locations, smooth type). See 'mgcv' and 'gamm'

We're going to leave GAMs alone for now, but there's lots of great references out there. Examples:

* Gavin Simpson's work with GAMs and time series [here](https://www.biorxiv.org/content/10.1101/322248v1)
* Simon Wood's [book](https://www.amazon.com/Generalized-Additive-Models-Introduction-Statistical/dp/1498728332/ref=asc_df_1498728332/?tag=hyprod-20&linkCode=df0&hvadid=312065538926&hvpos=1o1&hvnetw=g&hvrand=11863417304955012193&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9033315&hvtargid=pla-305424873573&psc=1)

## Gaussian processes for time series

Similarities between GAMs and GP models:

* GAMs and GP predictive models use reduced dimensionality (knots) to constrain flexibility

Differences:

* GAMs use smooth functions & knot locations to constrain how neighbors affect mean
* GP models use covariance function to control how much neighbors can influence eachother based on how far apart they are


