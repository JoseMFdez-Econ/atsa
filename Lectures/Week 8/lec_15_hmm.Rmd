---
title: "Semi- and non-parametric time series models"
author: "Eric Ward"
date: "26 Feb 2019"
output:
  ioslides_presentation:
    css: lecture_slides.css
  beamer_presentation: default
subtitle: FISH 507 â€“ Applied Time Series Analysis
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, 
                      message=FALSE, results='hide',
                      fig.align = 'center')
library(ggplot2)
library(MARSS)
library(dplyr)
```

## Overview of today's material

* Gaussian process models for time series

* Neural network models

* Empirical dynamic modeling

## Gaussian processes for time series

Last week, we discussed exponential smoothing and in lab touched on GAMs

* Both approaches are similar in that they borrow information from neighbors
* Exponential smoothing usually borrows information from past data for forecasting
* Generalized additive models (GAMs) usually borrow information from both future and past data

## Gaussian processes for time series

GAMs estimate the *trend* using a smooth function, 

$$E\left[ Y \right] ={ B }_{ 0 }+f(x)$$

where like regression, we assume $Y\sim Normal\left( E\left[ Y \right] ,\sigma  \right)$

* The smooth function approximates the trend at a smaller subset of locations (aka *knots*)
* The density and location of the knots can affect how 'wiggly' the function is

## Gaussian processes for time series

For data that are regularly spaced in time, this probably isn't a big deal

* For instance if we use a cubic spline (default) on the 'airmiles' dataset (n = 23), a function approximating the trend is estimated at 10 equally spaced locations (grey vertical lines).

```{r echo =FALSE, fig.height=3.5, fig.width=3.5}
data(airmiles)
df = data.frame("y"=airmiles, "t"=1937:1960)
gam_mod = mgcv::gam(y ~ s(t, bs='cr'), data=df)
df$pred = predict(gam_mod)
knots = data.frame("x"=gam_mod$smooth[[1]]$xp)
ggplot(df, aes(t,y)) + geom_point(col="red") +
  geom_line(aes(t,pred)) + 
  geom_vline(data=knots, aes(xintercept=x), col="grey70")
```

* This fit looks pretty reasonable

## Gaussian processes for time series

Let's try again, this time knocking a few holes in the data. Removing years 1950:1953 and 1955:1959, the knot locations are no longer equally spaced, and weighted more toward the locations of data points. 

* Greater spacing between knots = less flexibility, more uncertainty (you can look at the 'se.fit' part of predict output)

```{r echo =FALSE, fig.height=3.5, fig.width=3.5}
data(airmiles)
df = data.frame("y"=airmiles, "t"=1937:1960)
df = dplyr::filter(df,t %in% c(1950:1953,1955:1959)==FALSE)
gam_mod = mgcv::gam(y ~ s(t, bs='cr'), data=df)
df$pred = predict(gam_mod)
knots = data.frame("x"=gam_mod$smooth[[1]]$xp)
ggplot(df, aes(t,y)) + geom_point(col="red") +
  geom_line(aes(t,pred)) + geom_vline(data=knots, aes(xintercept=x), col="grey70")
```

## Gaussian processes for time series

Recapping, GAMs are estimating the underlying *trend* using a smooth function, 

$$E\left[ Y \right] ={ B }_{ 0 }+f(x)$$

* It's important to note that this underlying trend function $f(x)$ is modeling the **mean**
* Smooths are very flexible (with respect to # knots, locations, smooth type). See 'mgcv' and 'gamm'

We're going to leave GAMs alone for now, but there's lots of great references out there. Examples:

* Gavin Simpson's work with GAMs and time series [here](https://www.biorxiv.org/content/10.1101/322248v1)
* Simon Wood's [book](https://www.amazon.com/Generalized-Additive-Models-Introduction-Statistical/dp/1498728332/ref=asc_df_1498728332/?tag=hyprod-20&linkCode=df0&hvadid=312065538926&hvpos=1o1&hvnetw=g&hvrand=11863417304955012193&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9033315&hvtargid=pla-305424873573&psc=1)

## Gaussian processes for time series

Similarities between GAMs and GP models:

* GAMs and GP predictive models use reduced dimensionality (knots) to constrain flexibility

Differences:

* GAMs use smooth functions & knot locations to constrain how neighbors affect mean
* GP models use covariance function to control how much neighbors can influence eachother based on how far apart they are

## Gaussian processes for time series

We have some function we want to approximate
```{r}
set.seed(123)
df=data.frame(y = cumsum(rnorm(100)), x=1:100)
g = mgcv::gam(y ~ s(x), data=df)

plot(1:100, g$fitted.values, xlab="X", ylab="Y")
```

## Gaussian processes for time series

We could estimate the latent values at all observed locations
* What are the downsides to this?
```{r}
set.seed(123)
df=data.frame(y = cumsum(rnorm(100)), x=1:100)
g = mgcv::gam(y ~ s(x), data=df)

plot(1:100, g$fitted.values, xlab="X", ylab="Y")
points(1:100, g$fitted.values, col="blue", pch=16)
```

## Gaussian processes for time series

Instead, consider estimating them at a subset of points and extrapolating (aka Kriging)

* these locations are called the *knots*
* extrapolating to other locations = *predictive process model*

```{r}
set.seed(123)
df=data.frame(y = cumsum(rnorm(100)), x=1:100)
g = mgcv::gam(y ~ s(x), data=df)

plot(1:100, g$fitted.values, xlab="X", ylab="Y")
points(seq(1,100,5), g$fitted.values[seq(1,100,5)], col="blue", pch=16)
```

## Gaussian processes for time series

Lots of applications in Fisheries and Ecology

* Munch et al. 2005 [link](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.137.6285&rep=rep1&type=pdf)
* Munch et al. 2018 [link](https://onlinelibrary.wiley.com/doi/full/10.1111/faf.12304)

Especially with applications to spatial models 

* Latimer et al. 2009 [link](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1461-0248.2008.01270.x)
* Finley et al. 2017 [link](https://arxiv.org/abs/1702.00434)
* Gelfand et al. 2018 [link](https://www.sciencedirect.com/science/article/pii/S2211675316300033)
* Anderson et al. 2018 [link](https://esajournals.onlinelibrary.wiley.com/doi/pdf/10.1002/ecy.2403)

## Gaussian processes for time series

Several options for estimating *f(x)* at knot locations

* Common choice is random effects

Gaussian Process models use the covariance function, $\Sigma$

* e.g. Assume the random effects are MV Normal, e.g. $w \sim MVNormal(u, \Sigma)$

## Gaussian processes for time series

We could estimate elements of $\Sigma$ as unconstrained matrix (e.g. 'unconstrained' in MARSS)

* but that's a lot of parameters! ~ m*(m+1)/2

We could try to zero out some elements of $\Sigma$

* but this will cause problems: if ${x}_{1}$ and ${x}_{2}$ are correlated, and ${x}_{1}$ and ${x}_{3}$ are correlated, ${x}_{2}$ and ${x}_{3}$ have to be correlated too

## Gaussian processes for time series

Instead, we'll use a covariance function (aka kernel). Common choices are 

* Exponential 
* Squared-exponential (Gaussian)
* Matern
* Anisotropic functions

## Gaussian processes for time series

For example with the exponential function,

$${ \Sigma}_{i,j}={\sigma}^{2}exp\left( -{ d }_{ i,j }/\tau  \right)$$

* ${\sigma}^{2}$ is the variance parameter (estimated)

* ${ d }_{ i,j }$ is the distance between points, e.g. $|{x}_{i}-{x}_{j}|$

* distance could be distance in time, space, etc

* $\tau$ is a scaling parameter (estimated)

## Gaussian processes for time series

Question:

**For our exponential function, how do $\sigma$ and $\tau$ control 'wiggliness'?**

## Gaussian processes for time series

**For our exponential function, how do $\sigma$ and $\tau$ control 'wiggliness'?**

* Larger values of $\sigma$ introduce more variability between $f(x)$ at knot locations

* Larger values of $\tau$ will make the 'exp(...)' term closer to 1




