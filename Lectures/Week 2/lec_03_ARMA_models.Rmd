---
title: "Intro to ARMA models"
subtitle: "FISH 507 â€“ Applied Time Series Analysis"
author: "Mark Scheuerell"
date: "15 Jan 2019"
output:
  ioslides_presentation:
    css: lecture_slides.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
set.seed(123)
```

## Topics for today

Review

* White noise  

* Random walks  

Autoregressive (AR) models

Moving average (MA) models

Autoregressive moving average (ARMA) models

Using ACF & PACF for model ID


## White noise (WN)

A time series $\{w_t\}$ is discrete white noise if its values are

1. independent  

2. identically distributed with a mean of zero


## White noise (WN)

A time series $\{w_t\}$ is discrete white noise if its values are

1. independent  

2. identically distributed with a mean of zero

Note that distributional form for $\{w_t\}$ is flexible


## White noise (WN)

```{r, fig.cap="$w_t = 2e_t - 1; e_t \\sim \\text{Bernoulli}(0.5)$"}
par(mfrow = c(1,2), mai = c(1.5,0.9,0.1,0.1), omi = c(0,0,0,0))
tt <- rbinom(100, 1, 0.5) * 2 - 1
plot.ts(tt, ylab = expression(italic(w[t])))
acf(tt)
```


## Gaussian white noise

We often assume so-called _Gaussian white noise_, whereby

$$
w_t \sim \text{N}(0,\sigma^2)
$$


## Gaussian white noise

We often assume so-called _Gaussian white noise_, whereby

$$
w_t \sim \text{N}(0,\sigma^2)
$$

and the following apply as well

&nbsp; &nbsp; autocovariance:&nbsp; $\gamma_k =
    \begin{cases}
      \sigma^2 & \text{if } k = 0 \\
      0 & \text{if } k \geq 1
    \end{cases}$

&nbsp; &nbsp; autocorrelation: &nbsp; $\rho_k =
    \begin{cases}
      1 & \text{if } k = 0 \\
      0 & \text{if } k \geq 1
    \end{cases}$


## Gaussian white noise

```{r ex_gaussian_wn, fig.cap="$w_t \\sim \\text{N}(0,1)$"}
par(mfrow = c(1,2), mai = c(1.5,0.9,0.1,0.1), omi = c(0,0,0,0))
tt <- rnorm(100)
plot.ts(tt, ylab = expression(italic(w[t])))
acf(tt)
```


## Random walk (RW)

A time series $\{x_t\}$ is a random walk if

1. $x_t = x_{t-1} + w_t$  

2. $w_t$ is white noise


## Random walk (RW)

The following apply to random walks

&nbsp; &nbsp; mean: &nbsp; $\mu_x = 0$

&nbsp; &nbsp; autocovariance: &nbsp; $\gamma_k(t) = t \sigma^2$

&nbsp; &nbsp; autocorrelation: &nbsp; $\rho_k(t) = \frac{t \sigma^2}{\sqrt{t \sigma^2(t + k) \sigma^2}}$


## Random walk (RW)

The following apply to random walks

&nbsp; &nbsp; mean: &nbsp; $\mu_x = 0$

&nbsp; &nbsp; autocovariance: &nbsp; $\gamma_k(t) = t \sigma^2$

&nbsp; &nbsp; autocorrelation: &nbsp; $\rho_k(t) = \frac{t \sigma^2}{\sqrt{t \sigma^2(t + k) \sigma^2}}$

_Note_: Random walks are not stationary


## Random walk (RW)

```{r ex_rw, fig.cap="$x_t = x_{t-1} + w_t; w_t \\sim \\text{N}(0,1)$"}
par(mfrow = c(1,2), mai = c(1.5,0.9,0.1,0.1), omi = c(0,0,0,0))
tt <- cumsum(rnorm(100))
plot.ts(tt, ylab = expression(italic(x[t])))
acf(tt)
```


## Biased random walk

A _biased random walk_ (or _random walk with drift_) is written as

$$
x_t = x_{t-1} + u + w_t
$$  

where $u$ is the bias (drift) per time step and $w_t$ is white noise


## Biased random walk

```{r ex_biased_rw, fig.cap="$x_t = x_{t-1} + 1 + w_t; w_t \\sim \\text{N}(0,1)$"}
par(mfrow = c(1,2), mai = c(1.5,0.9,0.1,0.1), omi = c(0,0,0,0))
xx <- ww <- rnorm(100)
uu <- 1
for(t in 2:100) {
  xx[t] <- xx[t-1] + uu + ww[t]
}
plot.ts(xx, ylab = expression(italic(x[t])))
acf(tt)
```


## Differencing a biased random walk

First-differencing a biased random walk yields white noise with a non-zero mean $u$

$$
\begin{align}
  \nabla x_t &= x_{t-1} + u + w_t \\
  x_t - x_{t-1} &= x_{t-1} + u + w_t - x_{t-1} \\
  x_t - x_{t-1} &= u + w_t
\end{align}
$$


## Differencing a biased random walk

```{r ex_diff_biased_rw, fig.cap="$x_t = x_{t-1} + 1 + w_t; w_t \\sim \\text{N}(0,1)$"}
par(mfrow = c(1,2), mai = c(1.5,0.9,0.1,0.1), omi = c(0,0,0,0))
plot.ts(diff(xx), ylab = expression(nabla~italic(x[t])))
acf(tt)
```


## {.flexbox .vcenter}

<font size="10">LINEAR STATIONARY MODELS</font>


## Linear stationary models

We saw last week that linear filters are a useful way of modeling time series

Here we extend those ideas to a general class of models call _autoregressive moving average_ (ARMA) models


## Autoregressive (AR) models

An _autoregressive_ model of order _p_, or AR(_p_), is defined as

$$
x_t = \phi_1 x_{t-1} + \phi_2 x_{t-2} + \dots + \phi_p x_{t-p} + w_t
$$

where we assume

1. $w_t$ is white noise

2. $\phi_p \neq 0$ for an order-_p_ process


## Examples of AR(_p_) models

AR(1)

$x_t = 0.5 x_{t-1} + w_t$

<br>
Random walk ($\phi = 1$)

$x_t = x_{t-1} + w_t$


## Stationary AR(_p_) models

Recall that _stationary_ processes have the following properties

1. no systematic change in the mean or variance  
2. no systematic trend  
3. no periodic variations or seasonality

We seek a means for identifying whether our models are also stationary


## Stationary AR(_p_) models

We can write out an AR(_p_) model using the backshift operator ($\mathbf{B}$), such that


$$
  x_t = \phi_1 x_{t-1} + \phi_2 x_{t-2} + \dots + \phi_p x_{t-p} + w_t \\
  \Downarrow \\
\begin{align}
  x_t - \phi_1 x_{t-1} - \phi_2 x_{t-2} - \dots - \phi_p x_{t-p} &= w_t \\
  (1 - \phi_1 \mathbf{B} - \phi_2 \mathbf{B}^2 - \dots - \phi_p \mathbf{B}^p) x_t &= w_t \\
  \phi_p (\mathbf{B}) x_t &= w_t \\
\end{align}
$$

## Stationary AR(_p_) models

If we treat $\mathbf{B}$ as a number, we can out write the _characteristic equation_ as

$$
\phi_p (\mathbf{B}) x_t = w_t \\
\Downarrow \\
\phi_p (\mathbf{B}) = 0
$$

In order to be stationary, _all roots of the characteristic equation must exceed 1 in absolute value_


## Stationary AR(_p_) models

For example, consider this AR(1) model from earlier

$$
\begin{align}
  x_t &= 0.5 x_{t-1} + w_t \\
  x_t - 0.5 x_{t-1} &= w_t \\
  (1 - 0.5 \mathbf{B})x_t &= w_t \\
\end{align}
$$


## Stationary AR(_p_) models

For example, consider this AR(1) model from earlier

$$
\begin{align}
  x_t &= 0.5 x_{t-1} + w_t \\
  x_t - 0.5 x_{t-1} &= w_t \\
  (1 - 0.5 \mathbf{B})x_t &= w_t \\
  \Downarrow \\
  1 - 0.5 \mathbf{B} &= 0 \\
  -0.5 \mathbf{B} &= -1 \\
  \mathbf{B} &= 2 \\
\end{align}
$$

This model is indeed stationary because $\mathbf{B} > 1$


## What about random walks?

Are random walk models stationary?

$$
\begin{align}
  x_t &= x_{t-1} + w_t \\
  x_t - x_{t-1} &= w_t \\
  (1 - 1 \mathbf{B})x_t &= w_t \\
\end{align}
$$


## What about random walks?

Are random walk models stationary?

$$
\begin{align}
  x_t &= x_{t-1} + w_t \\
  x_t - x_{t-1} &= w_t \\
  (1 - 1 \mathbf{B})x_t &= w_t \\
  \Downarrow \\
  1 - 1 \mathbf{B} &= 0 \\
  -1 \mathbf{B} &= -1 \\
  \mathbf{B} &= 1 \\
\end{align}
$$

Random walks are __not__ stationary because $\mathbf{B} = 1 \ngtr 1$


## Topics for today

Review

* White noise  

* Random walks  

Autoregressive (AR) models

Moving average (MA) models

Autoregressive moving average (ARMA) models

Using ACF & PACF for model ID
